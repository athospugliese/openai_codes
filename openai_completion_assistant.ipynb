{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyParRJ1d8cyUkMQxHvXDc01"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gioEV1aaesAe",
        "outputId": "9d599099-2733-4405-89c7-7ae4c8b10998"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.16.2-py3-none-any.whl (267 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/267.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m266.2/267.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
      ],
      "metadata": {
        "id": "WvrJbCZ5fXtV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8fxPxkUdUbX",
        "outputId": "ab46507f-901d-4a4d-bbf4-446796279e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite o código em R: r\n",
            "```python\n",
            "# Python code will be here\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Solicita ao usuário que insira o código em R\n",
        "r_code = input(\"Digite o código em R: \")\n",
        "\n",
        "# Define o conteúdo da mensagem do usuário com base no código em R inserido\n",
        "user_message_content = f\"\"\"Convert the R code in the following markdown code block into Python. Pay particular attention to style, and try to use imports correctly. Render all output code between \"```python\" markdown code blocks:\n",
        "\n",
        "```R\n",
        "\n",
        "{r_code}\n",
        "\n",
        "```\"\"\"\n",
        "\n",
        "# Cria a mensagem do usuário\n",
        "user_message = {\"role\": \"user\", \"content\": user_message_content}\n",
        "\n",
        "# Adiciona a mensagem do usuário à lista de mensagens\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a highly experienced developer working on LangChain, an open source framework that helps developers build applications with large language models (LLMs).\\n\\nLangChain contains abstractions and integrations for working with LLMs, chat models, vector databases, autonomous agents, and more.\\n\\nIt has two versions: R, Python and PySpark. You are well-versed in both languages, and help keep both versions in sync.\"},\n",
        "    user_message\n",
        "]\n",
        "\n",
        "# Cria a conclusão com as mensagens atualizadas\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "\n",
        "# Imprime a resposta gerada\n",
        "print(completion.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "my_assistant = client.beta.assistants.create(\n",
        "    instructions=\"You are a personal python tutor. When asked a question, you change the default valures and run the code to answer the question.\",\n",
        "    name=\"Python Tutor\",\n",
        "    tools=[{\"type\": \"code_interpreter\"}],\n",
        "    model=\"gpt-4\",\n",
        ")\n",
        "print(my_assistant)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzRYAefxjHAY",
        "outputId": "6e014058-f5d2-406f-ea8f-4700d80ee2a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant(id='asst_2cWofrcIiBe0hOKQGWBgoP3P', created_at=1712627834, description=None, file_ids=[], instructions='You are a personal python tutor. When asked a question, you change the default valures and run the code to answer the question.', metadata={}, model='gpt-4', name='Python Tutor', object='assistant', tools=[CodeInterpreterTool(type='code_interpreter')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create()"
      ],
      "metadata": {
        "id": "GPBKFbcGkEXp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=completion.choices[0].message.content\n",
        ")"
      ],
      "metadata": {
        "id": "0e-EL1QekE_x"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import override\n",
        "from openai import AssistantEventHandler\n",
        "\n",
        "# First, we create a EventHandler class to define\n",
        "# how we want to handle the events in the response stream.\n",
        "\n",
        "class EventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_text_delta(self, delta, snapshot):\n",
        "    print(delta.value, end=\"\", flush=True)\n",
        "\n",
        "  def on_tool_call_created(self, tool_call):\n",
        "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
        "\n",
        "  def on_tool_call_delta(self, delta, snapshot):\n",
        "    if delta.type == 'code_interpreter':\n",
        "      if delta.code_interpreter.input:\n",
        "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
        "      if delta.code_interpreter.outputs:\n",
        "        print(f\"\\n\\noutput >\", flush=True)\n",
        "        for output in delta.code_interpreter.outputs:\n",
        "          if output.type == \"logs\":\n",
        "            print(f\"\\n{output.logs}\", flush=True)\n",
        "\n",
        "# Then, we use the `create_and_stream` SDK helper\n",
        "# with the `EventHandler` class to create the Run\n",
        "# and stream the response.\n",
        "\n",
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id='asst_2cWofrcIiBe0hOKQGWBgoP3P',\n",
        "  instructions=\"Please address the user as Jane Doe. The user has a premium account.\",\n",
        "  event_handler=EventHandler(),\n",
        ") as stream:\n",
        "  stream.until_done()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOw-h3tdkPEq",
        "outputId": "95db1068-29e4-42fc-f253-27151f6b82c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "assistant > Sure, Jane Doe. Feel free to provide the instruction or Python code that you want me to handle."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI, AssistantEventHandler\n",
        "from typing_extensions import override\n",
        "\n",
        "# Inicializa o cliente OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "# Solicita ao usuário que insira o código em R\n",
        "r_code = input(\"Digite o código em R: \")\n",
        "\n",
        "# Define o conteúdo da mensagem do usuário com base no código em R inserido\n",
        "user_message_content = f\"\"\"Convert the R code in the following markdown code block into Python. Pay particular attention to style, and try to use imports correctly. Render all output code between \"```python\" markdown code blocks:\n",
        "\n",
        "```R\n",
        "\n",
        "{r_code}\n",
        "\n",
        "```\"\"\"\n",
        "\n",
        "# Cria a mensagem do usuário\n",
        "user_message = {\"role\": \"user\", \"content\": user_message_content}\n",
        "\n",
        "# Adiciona a mensagem do usuário à lista de mensagens\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a highly experienced developer working on LangChain, an open source framework that helps developers build applications with large language models (LLMs).\\n\\nLangChain contains abstractions and integrations for working with LLMs, chat models, vector databases, autonomous agents, and more.\\n\\nIt has two versions: R, Python and PySpark. You are well-versed in both languages, and help keep both versions in sync.\"},\n",
        "    user_message\n",
        "]\n",
        "\n",
        "# Cria a conclusão com as mensagens atualizadas\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# Imprime a resposta gerada pelo ChatGPT\n",
        "print(completion.choices[0].message.content)\n",
        "\n",
        "code_python = completion.choices[0].message.content\n",
        "\n",
        "# Cria um assistente com base nas instruções fornecidas\n",
        "my_assistant = client.beta.assistants.create(\n",
        "    instructions=\"You are a personal python tutor. When asked a question, you change the default values and run the code to answer the question.\",\n",
        "    name=\"Python Tutor\",\n",
        "    tools=[{\"type\": \"code_interpreter\"}],\n",
        "    model=\"gpt-4\",\n",
        ")\n",
        "\n",
        "# Inicializa o ID do assistente criado\n",
        "assistant_id = my_assistant.id\n",
        "\n",
        "# Define o ID do thread\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "# Cria uma mensagem com o conteúdo gerado pelo modelo anterior\n",
        "message_content = completion.choices[0].message.content\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=message_content\n",
        ")\n",
        "\n",
        "# Define um manipulador de eventos personalizado para o assistente\n",
        "class EventHandler(AssistantEventHandler):\n",
        "    @override\n",
        "    def on_text_created(self, text) -> None:\n",
        "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "    @override\n",
        "    def on_text_delta(self, delta, snapshot):\n",
        "        print(delta.value, end=\"\", flush=True)\n",
        "\n",
        "    def on_tool_call_created(self, tool_call):\n",
        "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
        "\n",
        "    def on_tool_call_delta(self, delta, snapshot):\n",
        "        if delta.type == 'code_interpreter':\n",
        "            if delta.code_interpreter.input:\n",
        "                print(delta.code_interpreter.input, end=\"\", flush=True)\n",
        "            if delta.code_interpreter.outputs:\n",
        "                print(f\"\\n\\noutput >\", flush=True)\n",
        "                for output in delta.code_interpreter.outputs:\n",
        "                    if output.type == \"logs\":\n",
        "                        print(f\"\\n{output.logs}\", flush=True)\n",
        "\n",
        "# Cria o manipulador de eventos\n",
        "event_handler = EventHandler()\n",
        "\n",
        "# Inicia o stream de execução do assistente\n",
        "with client.beta.threads.runs.stream(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id,\n",
        "    instructions=\"Change the default values from {}, and do python tests and print the test and the result.\".format(python_code),\n",
        "    event_handler=event_handler,\n",
        ") as stream:\n",
        "    stream.until_done()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl66AflNl2Dq",
        "outputId": "dedaa915-6497-421d-c66e-d45834ad9525"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite o código em R: paises <- data.frame(   nome = c(\"Brasil\", \"Argentina\", \"Chile\", \"Uruguai\", \"Paraguai\"),   populacao = c(211000000, 45100000, 19100000, 3510000, 7130000),   area_km2 = c(8515767, 2780400, 756102, 181034, 406752),   idh = c(0.765, 0.830, 0.851, 0.817, 0.695) )  # Visualiza as primeiras linhas do dataframe print(\"Primeiras linhas do dataframe:\") print(head(paises))\n",
            "```python\n",
            "import pandas as pd\n",
            "\n",
            "data = {\n",
            "    'nome': ['Brasil', 'Argentina', 'Chile', 'Uruguai', 'Paraguai'],\n",
            "    'populacao': [211000000, 45100000, 19100000, 3510000, 7130000],\n",
            "    'area_km2': [8515767, 2780400, 756102, 181034, 406752],\n",
            "    'idh': [0.765, 0.830, 0.851, 0.817, 0.695]\n",
            "}\n",
            "\n",
            "paises = pd.DataFrame(data)\n",
            "\n",
            "print(\"Primeiras linhas do dataframe:\")\n",
            "print(paises.head())\n",
            "```  \n",
            "\n",
            "assistant > code_interpreter\n",
            "\n",
            "import pandas as pd\n",
            "\n",
            "data = {\n",
            "    'nome': ['Brasil', 'Argentina', 'Chile', 'Uruguai', 'Paraguai'],\n",
            "    'populacao': [211000000, 45100000, 19100000, 3510000, 7130000],\n",
            "    'area_km2': [8515767, 2780400, 756102, 181034, 406752],\n",
            "    'idh': [0.765, 0.830, 0.851, 0.817, 0.695]\n",
            "}\n",
            "\n",
            "paises = pd.DataFrame(data)\n",
            "\n",
            "print(\"Primeiras linhas do dataframe:\")\n",
            "print(paises.head())\n",
            "\n",
            "output >\n",
            "\n",
            "Primeiras linhas do dataframe:\n",
            "        nome  populacao  area_km2    idh\n",
            "0     Brasil  211000000   8515767  0.765\n",
            "1  Argentina   45100000   2780400  0.830\n",
            "2      Chile   19100000    756102  0.851\n",
            "3    Uruguai    3510000    181034  0.817\n",
            "4   Paraguai    7130000    406752  0.695\n",
            "\n",
            "\n",
            "assistant > The first lines of the dataframe are:\n",
            "\n",
            "|   |nome       |populacao |area_km2 |idh   |\n",
            "|---|-----------|----------|---------|------|\n",
            "|0  |Brasil     |211000000 |8515767  |0.765 |\n",
            "|1  |Argentina  |45100000  |2780400  |0.830 |\n",
            "|2  |Chile      |19100000  |756102   |0.851 |\n",
            "|3  |Uruguai    |3510000   |181034   |0.817 |\n",
            "|4  |Paraguai   |7130000   |406752   |0.695 |"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5HXVCySMnkPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}